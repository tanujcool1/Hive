
Scenario Based questions:

Q Will the reducer work or not if you use “Limit 1” in any HiveQL query?
A **"yes reducer will work as limit function only works after entire data has been quryed and shows random result with only one row."**


Q Suppose I have installed Apache Hive on top of my Hadoop cluster using default metastore configuration. Then, what will happen if we have multiple clients trying to access Hive at the same time? 
A **multiple users will not be able to access the hive metastore at same time and will get an error.**

Q Suppose, I create a table that contains details of all the transactions done by the customers: CREATE TABLE transaction_details (cust_id INT, amount FLOAT, month STRING, country STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ ;
Now, after inserting 50,000 records in this table, I want to know the total revenue generated for each month. But, Hive is taking too much time in processing this query. How will you solve this problem and list the steps that I will be taking in order to do so?
A  **1 create a table partitioned by month**
      CREATE TABLE partitioned_table
      (
         cust_id int,
         amount float,
         month string,
         country string
      )
      PARTITIONED BY(month string)
      row format delimited
      fields terminated by (,);
     **2 enable dynamic partitioning in hive** 
        SET hive.exec.dynamic.partition = true;
        SET hive.exec.dynamic.partition.mode = nonstrict;
   **3 transfer data from non partitioned to partiotioned table and query based on per month bases** 
      insert overwrite table partitioned_table 
      partition(month) 
      SELECT cust_id, amount, month, country FROM transaction_details;

Q How can you add a new partition for the month December in the above partitioned table?
A     ALTER TABLE partitioned_table 
      ADD PARTITION (month = dec) 
      LOCATION partitioned_table;

Q I am inserting data into a table based on partitions dynamically. But, I received an error – FAILED ERROR IN SEMANTIC ANALYSIS: Dynamic partition strict mode requires at least one static partition column. How will you remove this error?
A **enable dyanamic partition and then make it nonstrict** 



Q  Suppose, I have a CSV file – ‘sample.csv’ present in ‘/temp’ directory with the following entries:
id first_name last_name email gender ip_address
How will you consume this CSV file into the Hive warehouse using built-in SerDe?

A
CREATE EXTERNAL TABLE sample_csv
(
   id int,
   first_name string,
   last_name string,
   email string,
   gender string,
   ip_address string
)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
STORED AS textfile
LOCATION /temp;



Suppose, I have a lot of small CSV files present in the input directory in HDFS and I want to create a single Hive table corresponding to these files. The data in these files are in the format: {id, name, e-mail, country}. Now, as we know, Hadoop performance degrades when we use lots of small files.
So, how will you solve this problem where we want to create a single Hive table for lots of small files without degrading the performance of the system?



Q LOAD DATA LOCAL INPATH ‘Home/country/state/’
OVERWRITE INTO TABLE address;
The following statement failed to execute. What can be the cause?
A **local inpath should cointain file name directory folder works for hdfs file**

Q Is it possible to add 100 nodes when we already have 100 nodes in Hive? If yes, how?















Hive Practical questions:

Hive Join operations

Q Create a  table named CUSTOMERS(ID | NAME | AGE | ADDRESS   | SALARY)
Create a Second  table ORDER(OID | DATE | CUSTOMER_ID | AMOUNT
)

Now perform different joins operations on top of these tables
(Inner JOIN, LEFT OUTER JOIN ,RIGHT OUTER JOIN ,FULL OUTER JOIN)
A  


BUILD A DATA PIPELINE WITH HIVE

Download a data from the given location - 
https://archive.ics.uci.edu/ml/machine-learning-databases/00360/

1. Create a hive table as per given schema in your dataset                             **OK**

create database hive_assignment;
use hive_assignment;

**create csv textfile file**
CREATE TABLE air_quality_csv
(
date_data date,
time_data time,
CO_GT string,
PT08_S1_CO bigint,
NMHC_GT bigint,
C6H6_GT double,
PT08_S2_NMHC bigint,
NOx_GT bigint,
PT08_S3_NOx bigint,
NO2_GT bigint,
PT08_S4_NO2 bigint,
PT08_S5_O3 bigint,
T double,
RH double,
AH double
)
row format delimited
fields terminated by ','
stored as textfile
SET TBLPROPERTIES ("skip.header.line.count" = "1");

**create orc table schema**

CREATE TABLE air_quality_orc
(
date_data date,
time_data time,
CO_GT string,
PT08_S1_CO bigint,
NMHC_GT bigint,
C6H6_GT double,
PT08_S2_NMHC bigint,
NOx_GT bigint,
PT08_S3_NOx bigint,
NO2_GT bigint,
PT08_S4_NO2 bigint,
PT08_S5_O3 bigint,
T double,
RH double,
AH double
)
row format delimited
fields terminated by ','
stored as orc
SET TBLPROPERTIES ("skip.header.line.count" = "1");


2. try to place a data into table location.                                            **OK**

   load data local inpath 'file:///tmp/tanuj/AirQualityUCI.csv' into table air_quality_csv;

**to load data from text file to orc table**

   INSERT INTO TABLE air_quality_orc SELECT * FROM air_quality_csv;

3. Perform a select operation .                                                        **OK**

   SELECT * from air_quality_orc limit 10;


4. Fetch the result of the select operation in your local as a csv file .              **OK**

   SELECT AVG(CO_GT) FROM air_quality_orc;


5. Perform group by operation .                                                        **OK**

   SELECT SUM(CO_GT), date FROM air_quality_orc WHERE PT08_S1_CO > 1000 GROUP BY date LIMIT 100;


7. Perform filter operation at least 5 kinds of filter examples .                      **OK**

   a. distinct

      SELECT DISTINCT NMHC_GT
      FROM air_quality_orc;

   b. AVG

      SELECT AVG(CO_GT) FROM air_quality_orc;

   c. WHERE

      SELECT SUM(CO_GT), date_data FROM air_quality_orc WHERE PT08_S1_CO > 1000;

   d. Between

      SELECT SUM(CO_GT), date FROM air_quality_orc WHERE date_data BETWEEN '25-03-2004' AND '22-10-2004';

   e. IS NOT NULL

      SELECT SUM(CO_GT), date FROM air_quality_orc WHERE date_data IS NOT NULL LIMIT 100;

8. show and example of regex operation
9. alter table operation                                                               **OK**

   ALTER TABLE air_quality_orc 
   CHANGE COLUMN C6H6_GT C6H6_GT bigint;


10 . drop table operation                                                              **OK**

   DROP TABLE  air_quality_orc;


12 . order by operation .                                                              **OK** 

   SELECT SUM(CO_GT), date FROM air_quality_orc WHERE date_data IS NOT NULL LIMIT 100 ORDER BY CO_GT desc;


13 . where clause operations you have to perform .                                     **OK** 

   SELECT SUM(CO_GT), date_data FROM air_quality_orc WHERE PT08_S1_CO > 1000;


14 . sorting operation you have to perform .                                           **OK**

   SELECT SUM(CO_GT), date FROM air_quality_orc WHERE PT08_S1_CO > 1000 GROUP BY date  SORT BY CO_GT desc LIMIT 100;


15 . distinct operation you have to perform .                                          **OK**

   SELECT DISTINCT NMHC_GT
      FROM air_quality_orc;

      
16 . like an operation you have to perform . 
17 . union operation you have to perform . 
18 . table view operation you have to perform . 






hive operation with python

Create a python application that connects to the Hive database for extracting data, creating sub tables for data processing, drops temporary tables.fetch rows to python itself into a list of tuples and mimic the join or filter operations
